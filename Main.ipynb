{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-service",
   "metadata": {
    "id": "medieval-service"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SGD' from 'keras.optimizers' (C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c22df9e04245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SGD' from 'keras.optimizers' (C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U5a0PfsNWSZA",
   "metadata": {
    "id": "U5a0PfsNWSZA"
   },
   "outputs": [],
   "source": [
    "## Set flags\n",
    "\n",
    "# Set value to True to upload results to TensorBoard\n",
    "TENSORBOARD = False\n",
    "# Set value to True to have the neural network save its weights\n",
    "SAVEMODE = False\n",
    "# Set value to the path to the directory where the neural network will load its weights\n",
    "# if not loading set to None\n",
    "LOADDIR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-accessory",
   "metadata": {
    "id": "blocked-accessory"
   },
   "outputs": [],
   "source": [
    "def preprocessDataset(dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses an entire set of datapoints so that they are usable by a neural network\n",
    "\n",
    "    :param dataset: The dataset that is to be preprocessed in the form (images, labels)\n",
    "    :return: The preprocessed dataset in the form (processedImages, processedLabels)\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpacks the dataset into the images (input data) and labels (expected output data)\n",
    "    images, labels = dataset\n",
    "\n",
    "    # Preprocesses the input data\n",
    "    newImages = images.reshape(images.shape[0], images.shape[1], images.shape[2], 3)\n",
    "    # Condenses the values of the images so that they fall within 0 and 1\n",
    "    newImages = newImages / 255.0\n",
    "\n",
    "    # Preprocesses the expected output data\n",
    "\n",
    "    # Collapses the n-dimensional array into a 1D array\n",
    "    # For example:\n",
    "    # [ [ 1, 2 ], [ 3, 4 ] ]\n",
    "    # [ 1, 2, 3, 4 ]\n",
    "    newLabels = labels.flatten()\n",
    "\n",
    "    # Converts the labels from a 1D array of numbers ranging from 1 - 10 to a 2D array of 0s\n",
    "    # in which each number in the 1D array has a respective array within the 2D array, and\n",
    "    # the value of said number is the index in the other array that is a 1\n",
    "    #\n",
    "    # For example:\n",
    "    # 1D array: [ 1, 2, 2, 4, 3 ]\n",
    "    # One Hotted 2D Array:\n",
    "    # [ [ 1, 0, 0, 0 ],   // 1\n",
    "    #   [ 0, 1, 0, 0 ],   // 2\n",
    "    #   [ 0, 1, 0, 0 ],   // 2\n",
    "    #   [ 0, 0, 0, 1 ],   // 4\n",
    "    #   [ 0, 0, 1, 0 ] ]  // 3\n",
    "    newLabels = tf.one_hot(newLabels.astype(np.int32), depth=10)\n",
    "\n",
    "    return newImages, newLabels\n",
    "\n",
    "\n",
    "def getRawDatasets():\n",
    "    \"\"\"\n",
    "    :return: The unprocessed datasets that the neural network is to be trained on and tested against\n",
    "    \"\"\"\n",
    "\n",
    "    # Using the CIFAR-10 dataset\n",
    "    return tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-rough",
   "metadata": {
    "id": "international-rough"
   },
   "outputs": [],
   "source": [
    "def getTrainingConfigurations():\n",
    "    \"\"\"\n",
    "    Epochs: The amount of times the neural network trains against the entire training dataset\n",
    "    Batch Size: The amount of samples ran through before the gradient update is applied\n",
    "    Callbacks: The set of callbacks that will be applied during the training (e.g. saving)\n",
    "    Optimizer: The method that will be used in training the neural network\n",
    "    Loss Function: The loss function the neural network will use to judge its performance\n",
    "    Metrics: The metrics that will allow the user to understand the performance of the neural network\n",
    "\n",
    "    :param saveDir: The directory where the neural network is saved, indicated only if loading a neural network\n",
    "    :return: A tuple of the configurations described above, in order\n",
    "    \"\"\"\n",
    "\n",
    "    epochs = 300\n",
    "    batchSize = 32\n",
    "\n",
    "    # Gets the datetime now as to create unique folders\n",
    "    timeNow = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # The folder the fitness logs will be saved to\n",
    "    logDir = \"logs/fit/\" + timeNow\n",
    "    # The directory the neural network will be saved to\n",
    "    saveDir = \"save_files/\" + timeNow\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logDir, histogram_freq=1)]\n",
    "\n",
    "    if SAVEMODE:\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath=saveDir, verbose=1))\n",
    "\n",
    "    # Using Stochastic Gradient Descent, which estimates the gradient of the entire dataset\n",
    "    # instead of calculating the gradient at the current data point\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Calculates the loss based on the difference in value of the expected and actual outputs, with\n",
    "    # respect to the other outputs\n",
    "    loss = \"categorical_crossentropy\"\n",
    "\n",
    "    # Accuracy is being used as a user-friendly way of displaying the effectiveness of the neural network\n",
    "    metrics = [\"accuracy\"]\n",
    "\n",
    "    return epochs, batchSize, callbacks, optimizer, loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qqDyhKyOOYN",
   "metadata": {
    "id": "7qqDyhKyOOYN"
   },
   "outputs": [],
   "source": [
    "def createModel(train_images):\n",
    "  \"\"\"\n",
    "  Creates and returns the model of our CNN\n",
    "  Num_classes: The number of classes (categories) in the CIFAR-10 dataset\n",
    "  Model: The model of our CNN to be returned\n",
    "\n",
    "  #TODO: Make this more elegant??\n",
    "  :param: Set of training images, used to determine the input shape\n",
    "  :return: An object of type tf.keras.Model\n",
    "  \"\"\"\n",
    "  # 10 categories in the CIFAR-10 dataset\n",
    "  num_classes = 10\n",
    "\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Conv2D(32, 3, padding='same', input_shape=train_images.shape[1:], activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(layers.Dropout(0.25))\n",
    "\n",
    "  model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(layers.Dropout(0.45)) # was 0.35\n",
    "\n",
    "  model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Conv2D(128, 3, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(layers.Dropout(0.5)) # was 0.45\n",
    "\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.L2(0.001))) # added new regularizer\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-blogger",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "amazing-blogger",
    "outputId": "18e335d4-4fde-4fad-ec00-9dd57c6b34ad"
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Training configurations\n",
    "    epochs, batchSize, callbacks, optimizer, loss, metrics = getTrainingConfigurations()\n",
    "\n",
    "    # Raw datasets\n",
    "    rawTrainingDataset, rawTestingDataset = getRawDatasets()\n",
    "\n",
    "    # Preprocessed datasets\n",
    "    trainingImages, trainingLabels = preprocessDataset(rawTrainingDataset)\n",
    "    testingImages, testingLabels = preprocessDataset(rawTestingDataset)\n",
    "\n",
    "    # Model\n",
    "    model = createModel(trainingImages)\n",
    "\n",
    "    # Compile model, uses cross entropy as a loss function\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Set up TensorBoard\n",
    "    log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    if LOADDIR is not None:\n",
    "        model.load_weights(LOADDIR)\n",
    "    \n",
    "    # Begin training the model\n",
    "    history = model.fit(trainingImages, trainingLabels, epochs=epochs, validation_data=(testingImages, testingLabels), batch_size=batchSize, callbacks=callbacks)\n",
    "\n",
    "    # TensorBoard upload\n",
    "    if TENSORBOARD:\n",
    "      !tensorboard dev upload --logdir ./logs \\\n",
    "        --name \"INT2 Group 28 CIFAR-10\" \\\n",
    "        --description \"Training results from https://github.com/UOY-Int2-Group-28/Final-Project\" \\\n",
    "        --one_shot\n",
    "\n",
    "    # Create plot\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(testingImages, testingLabels, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
